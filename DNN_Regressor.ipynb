{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masterthesis\n",
    "#### Julian Jetz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-d642765712c8>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-d642765712c8>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    import tensorflow tf\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow tf\n",
    "from tensorflow.python.data import Dataset\n",
    "\n",
    "from scipy import stats\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from pandas.plotting import scatter_matrix\n",
    "from currency_converter import CurrencyConverter\n",
    "from datetime import date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_values = ['nan', 'N/A', 'NaN', 'NaT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('resources/accident_data.csv', sep=\";\", na_values=na_values, index_col=False, dtype = {\"STATE\" : \"str\", \"TYPE\" : \"str\", \"TYPEQ\" : \"str\", \"WEATHER\" : \"str\", \"VISIBLTY\" : \"str\"})\n",
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dev = 2\n",
    "df = df[(np.abs(stats.zscore(df[['LOCOMOTIVES1','LOADF1', 'LOADP1', 'EMPTYF1', 'EMPTYP1','LOCOMOTIVES2','LOADF2', 'EMPTYF2', 'INFRASTRUCTURE_DMG']])) < float(std_dev)).all(axis=1)]\n",
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "#df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f7c4552eb269>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ACCIDENT_TYPE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df[df['ACCIDENT_TYPE'].isnull()].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['EQUIPMENT_TYPE'].isnull()].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['WEATHER'].isnull()].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['VISIBLTY'].isnull()].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Jahreszeit'].isnull()].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['TONS']==0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['SPEED']==0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Adam Optimizer](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam) <br>\n",
    "Die Adam-Optimierung ist eine stochastische Gradientenabsenkungsmethode, die auf einer adaptiven Schätzung von Momenten erster und zweiter Ordnung basiert. Das Verfahren ist \"recheneffizient, hat wenig Speicherbedarf, ist invariant gegenüber der diagonalen Neuskalierung von Gradienten und eignet sich gut für Probleme, die in Bezug auf Daten/Parameter groß sind\".\n",
    "[Arxiv](https://arxiv.org/pdf/1412.6980.pdf)<br><br>\n",
    "*Learning rate:* In der maschinellen Lern- und Statistiktechnik ist die Lernrate ein Tuningparameter in einem Optimierungsalgorithmus, der die Schrittweite bei jeder Iteration bestimmt und sich dabei auf ein Minimum einer Verlustfunktion zubewegt. Da sie beeinflusst, inwieweit neu gewonnene Informationen alte Informationen übersteuern, stellt sie metaphorisch die Geschwindigkeit dar, mit der ein maschinelles Lernmodell \"lernt\". Bei der Festlegung einer Lernrate gibt es einen Kompromiss zwischen der Konvergenzrate und der Überschreitung. Während die Richtung zum Minimum in der Regel aus dem Gradienten der Verlustfunktion bestimmt wird, bestimmt die Lernrate, wie groß ein Schritt in diese Richtung ist.Eine zu hohe Lernrate führt dazu, dass der Lernsprung über Minima hinausgeht, aber eine zu niedrige Lernrate dauert entweder zu lange, um sich zu konvergieren oder in einem unerwünschten lokalen Minimum stecken zu bleiben.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opti = tf.train.AdamOptimizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Festlegen der abhängigen Variablen X und der vorherzusagenden Variable y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = df[['YEAR4', 'MONTH', 'LOCOMOTIVES1', 'LOADF1', 'LOADP1', 'EMPTYF1', 'EMPTYP1','LOCOMOTIVES2', 'LOADF2', 'EMPTYF2', 'TONS', 'TEMP_CLUSTER', 'SPEED', 'ACCCAUSE_LVL1', 'TYPE', 'EQUIPMENT_TYPE', 'TYPTRK', 'VISIBLTY', 'WEATHER', 'STATE', 'SPD_TOO_HIGH', 'ACCTYPE', 'ACCIDENT_TYPE', 'Jahreszeit']]\n",
    "y = df['INFRASTRUCTURE_DMG']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aufteilen der Daten in Train und Test Datensatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Festlegen der numerischen Merkmalsspalten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trnspd = tf.feature_column.numeric_column('SPEED')\n",
    "year = tf.feature_column.numeric_column('YEAR4')\n",
    "locomotives1 = tf.feature_column.numeric_column('LOCOMOTIVES1')\n",
    "loadf1 = tf.feature_column.numeric_column('LOADF1')\n",
    "loadp1 = tf.feature_column.numeric_column('LOADP1')\n",
    "emptyf1 = tf.feature_column.numeric_column('EMPTYF1')\n",
    "emptyp1 = tf.feature_column.numeric_column('EMPTYP1')\n",
    "locomotives2 = tf.feature_column.numeric_column('LOCOMOTIVES2')\n",
    "loadf2 = tf.feature_column.numeric_column('LOADF2')\n",
    "emptyf2 = tf.feature_column.numeric_column('EMPTYF2')\n",
    "tons = tf.feature_column.numeric_column('TONS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data[['ACCCAUSE_LVL1', 'TYPE', 'EQUIPMENT_TYPE', 'TYPTRK', 'VISIBLTY', 'WEATHER', 'STATE', 'SPD_TOO_HIGH', 'ACCTYPE', 'ACCIDENT_TYPE', 'TEMP_CLUSTER', 'Jahreszeit']].astype(str).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Festlegen der kategorischen Merkmalsspalten. \n",
    "Anstatt die Daten als einen one-hot Vektor mit vielen Dimensionen darzustellen, stellt eine Einbettungsspalte diese Daten als einen niederdimensionalen, dichten Vektor dar, in dem jede Zelle eine beliebige Zahl enthalten kann, nicht nur 0 oder 1. Die Größe der Einbettung ist ein Parameter, der angepasst werden muss (*TODO*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accause = tf.feature_column.categorical_column_with_identity('ACCCAUSE_LVL1',num_buckets=100000)\n",
    "\n",
    "embedding_size = int(math.floor(len(x_data['ACCCAUSE_LVL1'].unique())**0.25))\n",
    "accause=tf.feature_column.embedding_column(accause, dimension=embedding_size+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidenttype = tf.feature_column.categorical_column_with_identity('ACCIDENT_TYPE',num_buckets=100000)\n",
    "\n",
    "embedding_size = int(math.floor(len(x_data['ACCIDENT_TYPE'].unique())**0.25))\n",
    "accidenttype=tf.feature_column.embedding_column(accidenttype, dimension=embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typeq = tf.feature_column.categorical_column_with_identity('EQUIPMENT_TYPE',num_buckets=100000)\n",
    "\n",
    "embedding_size = int(math.floor(len(x_data['EQUIPMENT_TYPE'].unique())**0.25))\n",
    "typeq=tf.feature_column.embedding_column(typeq, dimension=embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typtrk = tf.feature_column.categorical_column_with_identity('TYPTRK',num_buckets=100000)\n",
    "\n",
    "embedding_size = int(math.floor(len(x_data['TYPTRK'].unique())**0.25))\n",
    "typtrk=tf.feature_column.embedding_column(typtrk, dimension=embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visibility = tf.feature_column.categorical_column_with_identity('VISIBLTY',num_buckets=100000)\n",
    "\n",
    "embedding_size = int(math.floor(len(x_data['VISIBLTY'].unique())**0.25))\n",
    "visibility=tf.feature_column.embedding_column(visibility, dimension=embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = tf.feature_column.categorical_column_with_identity('WEATHER',num_buckets=100000)\n",
    "\n",
    "embedding_size = int(math.floor(len(x_data['WEATHER'].unique())**0.25))\n",
    "weather=tf.feature_column.embedding_column(weather, dimension=embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = tf.feature_column.categorical_column_with_identity('STATE',num_buckets=100000)\n",
    "\n",
    "embedding_size = int(math.floor(len(x_data['STATE'].unique())**0.25))\n",
    "state=tf.feature_column.embedding_column(state, dimension=embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jahreszeit = tf.feature_column.categorical_column_with_identity('Jahreszeit',num_buckets=100000)\n",
    "\n",
    "embedding_size = int(math.floor(len(x_data['Jahreszeit'].unique())**0.25))\n",
    "jahreszeit=tf.feature_column.embedding_column(state, dimension=embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_cluster = tf.feature_column.categorical_column_with_identity('TEMP_CLUSTER',num_buckets=100000)\n",
    "\n",
    "embedding_size = int(math.floor(len(x_data['TEMP_CLUSTER'].unique())**0.25))\n",
    "temp_cluster=tf.feature_column.embedding_column(temp_cluster, dimension=embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spd_too_high = tf.feature_column.categorical_column_with_identity('SPD_TOO_HIGH',num_buckets=100000)\n",
    "\n",
    "embedding_size = int(math.floor(len(x_data['SPD_TOO_HIGH'].unique())**0.25))\n",
    "spd_too_high=tf.feature_column.embedding_column(spd_too_high, dimension=embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acctype = tf.feature_column.categorical_column_with_identity('ACCTYPE',num_buckets=100000)\n",
    "\n",
    "embedding_size = int(math.floor(len(x_data['ACCTYPE'].unique())**0.25))\n",
    "acctype=tf.feature_column.embedding_column(acctype, dimension=embedding_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Festlegen der Merkmalsspalten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_col =[year, trnspd, typeq, locomotives1, loadf1, loadp1, emptyf1, emptyp1, locomotives2, loadf2, emptyf2, typtrk, accidenttype, accause, visibility, weather, temp_cluster, state, jahreszeit, spd_too_high, acctype]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aufstellen der Input Funktion\n",
    "\n",
    "Batch_Size=Größe der zurückzusendenden Batches.<br>\n",
    "Num_Epochs=Anzahl der Perioden, die man über Daten iterieren muss.<br>\n",
    "Shuffle=Sollendie Datensätze in zufälliger Reihenfolge gelesen werden?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_func= tf.estimator.inputs.pandas_input_fn(x=x_train, y= y_train, batch_size=10, num_epochs=1000, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fun(features, targets, batch_size=1, shuffle=True, num_epochs=None):\n",
    "    features = {key:np.array(value) for key,value in dict(features).items()}                                             \n",
    " \n",
    "    # Construct a dataset, and configure batching/repeating.\n",
    "    shuffleset = tf.data.Dataset.from_tensor_slices((features,targets)) # warning: 2GB limit\n",
    "    shuffleset = shuffleset.batch(batch_size).repeat(num_epochs)\n",
    "    \n",
    "    # Shuffle the data, if specified.\n",
    "    if shuffle:\n",
    "        shuffleset = shuffleset.shuffle(10000)\n",
    "    \n",
    "    # Return the next batch of data.\n",
    "    iterator = shuffleset.__iter__()\n",
    "    next_element = iterator.get_next()\n",
    "    \n",
    "    features, labels = next_element\n",
    "    return features, labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_func = lambda: input_fun(features=x_train, targets=y_train, num_epochs=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aufstellen der Eval Input Funktion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_func = tf.estimator.inputs.pandas_input_fn(x=x_test,                                                   \n",
    "                                                 batch_size=10, \n",
    "                                                 num_epochs=1, \n",
    "                                                 shuffle=False)\n",
    "eval_input_func = tf.estimator.inputs.pandas_input_fn(x=x_test,\n",
    "                                                      y=y_test, \n",
    "                                                      batch_size=10, \n",
    "                                                      num_epochs=1, \n",
    "                                                      shuffle=False)\n",
    "train_input_func = tf.estimator.inputs.pandas_input_fn(x=x_train,                                                   \n",
    "                                                 batch_size=10, \n",
    "                                                 num_epochs=1, \n",
    "                                                 shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialisierung des Estimators (DNNRegressor)\n",
    "hidden_units=Das Argument hidden_units ermöglicht es, ein Array mit der Anzahl der Knoten für jede Schicht zu erzeugen. Dies ermöglicht es, ein neuronales Netzwerk zu erstellen, indem einfach seine Größe und Form berücksichtigt wird, anstatt das Ganze von Grund auf von Hand zu vernetzen. (TODO: fine tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#estimator = tf.estimator.DNNRegressor(hidden_units=[1024, 512, 256], feature_columns=feature_col, optimizer=lambda: tf.keras.optimizers.Adam(learning_rate=tf.compat.v1.train.exponential_decay(learning_rate=0.1,global_step=tf.compat.v1.train.get_global_step(),decay_steps=10000,decay_rate=0.96)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = tf.estimator.DNNRegressor(hidden_units=[1024, 512, 256], feature_columns=feature_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.train(input_fn=input_func, max_steps=60000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Model mit Eval input function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_eval = estimator.evaluate(input_fn=eval_input_func)\n",
    "result_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatterplot Vergleich tatsächliche und vorhergesagte Werte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=[]\n",
    "for pred in estimator.predict(input_fn=test_input_func):\n",
    "    predictions.append(pred['predictions'][0].astype(float))\n",
    "plt.plot(y_test, predictions, 'o')\n",
    "plt.xlabel('Actual values (test data)')\n",
    "plt.ylabel('predicted values (test data)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions=[]\n",
    "for pred in estimator.predict(input_fn=train_input_func):\n",
    "    train_predictions.append(pred['predictions'][0].astype(float))\n",
    "plt.plot(y_train, train_predictions, 'o')\n",
    "plt.xlabel('Actual values (train data)')\n",
    "plt.ylabel('predicted values (train data)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(mean_squared_error(y_test, predictions))**0.5\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verleich tatsächliche und vorhergesagte Werte \n",
    "Beispiel: 30 zufällig ausgewählte Werte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pd.DataFrame({'Actual': y_test, 'Predicted': predictions})\n",
    "pred1 = pred.sample(100)\n",
    "\n",
    "pred1.plot(kind='bar',figsize=(20,16))\n",
    "plt.grid(which='major', linestyle='-', linewidth='0.5', color='green')\n",
    "plt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Residual Plot \n",
    "Abweichung zwischen den vorhergesagten und tatsächlichen Unfallkosten (Testdaten=grün, Trainingsdaten=blau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    plt.scatter(train_predictions, train_predictions - y_train, c='b', s=40, alpha=0.5, label='Train Data')\n",
    "    plt.scatter(predictions, predictions - y_test, c='g', s=40, label='Test Data')\n",
    "    plt.hlines(y=0, xmin=-0.03, xmax=0.2)\n",
    "    plt.title('Residual Plot of DNN Regression')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.xlabel('Accident Damage')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Master)",
   "language": "python",
   "name": "masterthesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
